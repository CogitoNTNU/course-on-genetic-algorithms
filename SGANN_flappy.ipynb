{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Genetic algorithm for neuroevolution** \n",
    "\n",
    "This notebook aims to find a solution to flappy bird by using a simple genetic algorithm to conduct a large search to fit a neural network. The algorithm is based on genomes consisting of a bitstring, mapping the bits to a neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flappy-bird-gym in c:\\users\\tobia\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: gym~=0.18.0 in c:\\users\\tobia\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from flappy-bird-gym) (0.18.3)\n",
      "Requirement already satisfied: numpy~=1.19.5 in c:\\users\\tobia\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from flappy-bird-gym) (1.19.5)\n",
      "Requirement already satisfied: pygame~=2.0.1 in c:\\users\\tobia\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from flappy-bird-gym) (2.0.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\tobia\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gym~=0.18.0->flappy-bird-gym) (1.10.1)\n",
      "Requirement already satisfied: pyglet<=1.5.15,>=1.4.0 in c:\\users\\tobia\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gym~=0.18.0->flappy-bird-gym) (1.5.15)\n",
      "Requirement already satisfied: Pillow<=8.2.0 in c:\\users\\tobia\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gym~=0.18.0->flappy-bird-gym) (8.2.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\tobia\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gym~=0.18.0->flappy-bird-gym) (1.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\tobia\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "%pip install flappy-bird-gym\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import flappy_bird_gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a structure organism in order to keep track of the chromosome and the fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains functions for converting the chromosome into a neural network, as well as useful functions like forward pass in order to let the creatures make predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL!\n",
    "\n",
    "def bitstring_to_floats(bitstring, bits_per_value=10, min_val=-2, max_val=2):\n",
    "    \"\"\"Converts a bitstring into an array of float values while ensuring full extraction.\"\"\"\n",
    "    num_values = len(bitstring) // bits_per_value  # Ensure full number of weights\n",
    "    if num_values == 0:\n",
    "        raise ValueError(\"Bitstring too short for any weights!\")\n",
    "\n",
    "    floats = []\n",
    "    for i in range(num_values):\n",
    "        binary_segment = bitstring[i * bits_per_value:(i + 1) * bits_per_value]\n",
    "        decimal_value = int(binary_segment, 2)  # Convert binary to decimal\n",
    "        scaled_value = min_val + (max_val - min_val) * (decimal_value / (2**bits_per_value - 1))  # Normalize to [-2,2]\n",
    "        floats.append(scaled_value)\n",
    "\n",
    "    return np.array(floats)\n",
    "\n",
    "\n",
    "def decode_architecture(bitstring, bits_per_weight=10, input_size=2, output_size=1):\n",
    "    \"\"\"Extracts NN structure (neurons) and weights from a bitstring\"\"\"\n",
    "    hidden_neurons = int(bitstring[:4], 2) + 1  # Allow 1-16 hidden neurons\n",
    "    weights = bitstring_to_floats(bitstring[4:], bits_per_value=bits_per_weight)\n",
    "\n",
    "    # Compute expected number of weights\n",
    "    required_params = (input_size * hidden_neurons) + hidden_neurons + (hidden_neurons * output_size) + output_size\n",
    "\n",
    "    if len(weights) < required_params:\n",
    "        raise ValueError(f\"Decoded weights ({len(weights)}) are fewer than expected ({required_params}). \"\n",
    "                         f\"Ensure bitstring is at least {required_params * bits_per_weight} bits long!\")\n",
    "\n",
    "    return hidden_neurons, weights\n",
    "\n",
    "\n",
    "def construct_nn(bitstring, input_size=2, output_size=1):\n",
    "    \"\"\"Constructs a simple 1-layer NN from a bitstring with dynamic hidden neurons\"\"\"\n",
    "    hidden_neurons, params = decode_architecture(bitstring, input_size=input_size, output_size=output_size)\n",
    "\n",
    "    # Compute parameter indices\n",
    "    w1_end = input_size * hidden_neurons\n",
    "    b1_end = w1_end + hidden_neurons\n",
    "    w2_end = b1_end + (hidden_neurons * output_size)\n",
    "\n",
    "    # Debugging: Print out parameter sizes\n",
    "    # print(f\"Params Length: {len(params)} | Expected: {w2_end + output_size}\")\n",
    "    # print(f\"Hidden Neurons: {hidden_neurons}, Input Size: {input_size}, Output Size: {output_size}\")\n",
    "    # print(f\"w1: (0:{w1_end}), b1: ({w1_end}:{b1_end}), w2: ({b1_end}:{w2_end}), b2: ({w2_end}:{w2_end + output_size})\")\n",
    "\n",
    "    # Extract weights and biases\n",
    "    w1 = params[:w1_end].reshape(input_size, hidden_neurons)  # (input_size, hidden_neurons)\n",
    "    b1 = params[w1_end:b1_end]  # (hidden_neurons,)\n",
    "    w2 = params[b1_end:w2_end].reshape(hidden_neurons, output_size)  # (hidden_neurons, output_size)\n",
    "    b2 = params[w2_end:w2_end + output_size]  # (output_size,)\n",
    "\n",
    "    return w1, b1, w2, b2\n",
    "\n",
    "\n",
    "\n",
    "def forward_pass(X, w1, b1, w2, b2):\n",
    "    \"\"\"Performs forward propagation through the neural network\"\"\"\n",
    "    hidden = np.tanh(np.dot(X, w1) + b1)  # (batch, hidden)\n",
    "    output = np.tanh(np.dot(hidden, w2) + b2)  # (batch, 1)\n",
    "\n",
    "    return output\n",
    "\n",
    "def required_bitstring_length(hidden_neurons=16, input_size=2, output_size=1, bits_per_weight=10):\n",
    "    \"\"\"Compute the required bitstring length for encoding all NN parameters.\"\"\"\n",
    "    required_params = (input_size * hidden_neurons) + hidden_neurons + (hidden_neurons * output_size) + output_size\n",
    "    return required_params * bits_per_weight  # Convert to bitstring length\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load best genome of a run\n",
    "def save_best_genome(genome, filename):\n",
    "    \"\"\"Saves the best genome to a file\"\"\"\n",
    "    \n",
    "    with open (filename, 'w') as f:\n",
    "        f.write(genome)\n",
    "        \n",
    "def load_best_genome(filename):\n",
    "    \"\"\"Loads the best genome from a file\"\"\"\n",
    "    \n",
    "    with open (filename, 'r') as f:\n",
    "        genome = f.read()\n",
    "        \n",
    "    return genome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize population**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_population(pop_size, genome_length):\n",
    "    \"\"\"Initializes a population of genomes\"\"\"\n",
    "    return[\"\".join(np.random.choice([\"0\", \"1\"], genome_length)) for _ in range(pop_size)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "def test_initialize_population():\n",
    "    np.random.seed(42)\n",
    "    pop_size = 3\n",
    "    genome_length = 5\n",
    "    population = initialize_population(pop_size, genome_length)\n",
    "    \n",
    "    expected_population = ['01000', '10001', '00001']\n",
    "    \n",
    "    assert len(population) == pop_size, f\"Expected population size {pop_size}, got {len(population)}\"\n",
    "    \n",
    "    # Verify each genome has the correct length and contains only '0' and '1'\n",
    "    for genome in population:\n",
    "        assert len(genome) == genome_length, f\"Expected genome length {genome_length}, got {len(genome)}\"\n",
    "        for char in genome:\n",
    "            assert char in ['0', '1'], f\"Unexpected character {char} in genome\"\n",
    "\n",
    "    assert population == expected_population, f\"Expected {expected_population}, got {population}\"\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_initialize_population()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitness Function**\n",
    "The fitness of a genome is calculated in a fitness function. This often the only domain centric part of a genetic algorithm, meaning that the neural network will be trained based on what kind of environment the fitness function uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "\n",
    "def evaluate_fitness(chromosome):\n",
    "    \"\"\"Evaluates a chromosome based on its fitness.\"\"\"\n",
    "    hidden_neurons, params = decode_architecture(chromosome)\n",
    "    w1, b1, w2, b2 = construct_nn(chromosome)\n",
    "\n",
    "    # Run the game\n",
    "    obs, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        \n",
    "        try:\n",
    "            X = np.array(obs).reshape(1, 2)   \n",
    "        except:\n",
    "            X = np.array([0.1,1.0]).reshape(1, 2) # the first observation is not in the correct shape\n",
    "\n",
    "        # Forward pass through the NN\n",
    "        output = forward_pass(X, w1, b1, w2, b2)\n",
    "        \n",
    "        # TODO: play around with the threshold and see if you can get a better model\n",
    "        action = 1 if output > 0.7 else 0\n",
    "\n",
    "        # Take the action\n",
    "        obs, reward, terminated, info = env.step(action)\n",
    "        total_reward += reward - action * 7\n",
    "        \n",
    "        # Check if the game is over\n",
    "        if terminated:\n",
    "            break\n",
    "        \n",
    "    # clamp reward to be at least 0.000001\n",
    "    return max(total_reward, 0.000001) +  + int(info['score'])*1000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "# TEST \n",
    "def test_evaluate_fitness():\n",
    "    np.random.seed(42)\n",
    "    dummy_chromosome = \"1011000110\"*50\n",
    "    fitness = evaluate_fitness(dummy_chromosome)\n",
    "    assert isinstance(fitness, (int, float)), f\"Expected fitness to be a number, got {type(fitness)}\"\n",
    "    assert fitness >= 0, f\"Expected fitness to be non-negative, got {fitness}\"\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "test_evaluate_fitness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crossover**\n",
    "The crossover function decides how different genomes should mate in order to produce offspring. \n",
    "The gene representation is in the form of a bitstring.\n",
    "\n",
    "**Task** \n",
    "Define your own set of crossover functions by implementing different policies to combine the bitstirng of two parents. All genomes are the same length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_crossover(parent1, parent2):\n",
    "    \"\"\"Uniform crossover for binary genomes\"\"\"\n",
    "    mask = np.random.randint(2, size=len(parent1))  # Random bit mask\n",
    "    child1 = \"\".join([parent1[i] if mask[i] else parent2[i] for i in range(len(parent1))])\n",
    "    child2 = \"\".join([parent2[i] if mask[i] else parent1[i] for i in range(len(parent1))])\n",
    "    return child1, child2\n",
    "\n",
    "\n",
    "def single_point_crossover(parent1, parent2, crossover_rate=0.9):\n",
    "    \"\"\"Single-point crossover for binary genomes\"\"\"\n",
    "    if np.random.rand() > crossover_rate:\n",
    "        return parent1, parent2\n",
    "    split_point = np.random.randint(1, len(parent1) - 1)  # Random split point\n",
    "    child1 = parent1[:split_point] + parent2[split_point:]\n",
    "    child2 = parent2[:split_point] + parent1[split_point:]\n",
    "    return child1, child2\n",
    "\n",
    "\n",
    "def crossover(parent1, parent2, crossover_rate=0.9):\n",
    "    random_number = np.random.rand()\n",
    "    if random_number < 0.5:\n",
    "        return single_point_crossover(parent1, parent2, crossover_rate)\n",
    "    else:\n",
    "        return uniform_crossover(parent1, parent2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bit_flip_mutation(bitstring, mutation_rate=0.01):\n",
    "    \"\"\"Flips random bits with given probability\"\"\"\n",
    "    return \"\".join([bit if np.random.rand() > mutation_rate else str(1 - int(bit)) for bit in bitstring])\n",
    "\n",
    "\n",
    "def adaptive_mutate(chromosome, generation, max_generations, mutation_rate=0.01, diversity_factor=1):\n",
    "    \"\"\"Adapts mutation rate based on population diversity.\"\"\"\n",
    "    adjusted_mutation_rate = mutation_rate * (1 - (generation / max_generations)) * diversity_factor\n",
    "\n",
    "    chromosome_list = list(chromosome)  # Convert to mutable list\n",
    "    for i in range(len(chromosome_list)):\n",
    "        if np.random.rand() < adjusted_mutation_rate:\n",
    "            chromosome_list[i] = '1' if chromosome_list[i] == '0' else '0'  # Flip bit\n",
    "\n",
    "    return \"\".join(chromosome_list)  # Convert back to string\n",
    "\n",
    "\n",
    "def swap_mutation(chromosome):\n",
    "    \"\"\"Performs swap mutation on a given chromosome by swapping two random genes.\"\"\"\n",
    "    if len(chromosome) < 2:\n",
    "        return chromosome  # No mutation possible\n",
    "    \n",
    "    idx1, idx2 = random.sample(range(len(chromosome)), 2)\n",
    "    chromosome[idx1], chromosome[idx2] = chromosome[idx2], chromosome[idx1]\n",
    "    \n",
    "    return chromosome\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deterministic_crowding(parent1, parent2, child1, child2, fitness_function):\n",
    "    \"\"\"Ensures offspring compete with similar parents.\"\"\"\n",
    "    f_p1 = fitness_function(parent1)\n",
    "    f_p2 = fitness_function(parent2)\n",
    "    f_c1 = fitness_function(child1)\n",
    "    f_c2 = fitness_function(child2)\n",
    "\n",
    "    new1 = child1 if f_c1 > f_p1 else parent1\n",
    "    new2 = child2 if f_c2 > f_p2 else parent2\n",
    "\n",
    "    return new1, new2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_distance(bitstring1, bitstring2):\n",
    "    \"\"\"Calculates Hamming distance between two bitstrings.\"\"\"\n",
    "    return sum(b1 != b2 for b1, b2 in zip(bitstring1, bitstring2))\n",
    "\n",
    "\n",
    "def fitness_sharing(population, fitnesses, sigma_share=10, alpha=2):\n",
    "    \"\"\"Applies fitness sharing to promote diversity.\"\"\"\n",
    "    shared_fitnesses = np.zeros(len(fitnesses))\n",
    "\n",
    "    for i in range(len(population)):\n",
    "        niche_count = 0\n",
    "        for j in range(len(population)):\n",
    "            if i != j:\n",
    "                distance = hamming_distance(population[i], population[j])\n",
    "                if distance < sigma_share:  # If genomes are similar\n",
    "                    niche_count += (1 - (distance / sigma_share)) ** alpha\n",
    "\n",
    "        shared_fitnesses[i] = fitnesses[i] / (1 + niche_count)  # Penalize common solutions\n",
    "\n",
    "    return shared_fitnesses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tournament_selection(population, fitnesses, tournament_size=10):\n",
    "    \"\"\"Select best genome from a random subset\"\"\"\n",
    "    indices = np.random.choice(len(population), tournament_size, replace=False)\n",
    "    best_index = indices[np.argmax([fitnesses[i] for i in indices])]\n",
    "    return population[best_index]\n",
    "\n",
    "\n",
    "def roulette_wheel_selection(population, fitnesses):\n",
    "    \"\"\"Selects individuals using fitness proportionate selection.\"\"\"\n",
    "    total_fitness = np.sum(fitnesses)\n",
    "    probabilities = fitnesses / total_fitness\n",
    "    return population[np.random.choice(len(population), p=probabilities)]\n",
    "\n",
    "\n",
    "def rank_based_selection(population):\n",
    "    \"\"\"Selects a parent based on rank selection.\"\"\"\n",
    "    population_sorted = sorted(population, key=lambda org: org.fitness)\n",
    "    ranks = np.arange(1, len(population) + 1)\n",
    "    probabilities = ranks / ranks.sum()\n",
    "    return np.random.choice(population_sorted, p=probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genetic_algorithm(pop_size, genome_length, generations):\n",
    "    \"\"\"Runs a genetic algorithm to evolve a neural network\"\"\"\n",
    "    # Initialize population (random bitstrings)\n",
    "    population = initialize_population(pop_size, genome_length)\n",
    "    max_generation = population\n",
    "\n",
    "    for gen in range(generations):\n",
    "        fitnesses = np.array([evaluate_fitness(ind) for ind in population])\n",
    "        fitnesses = fitness_sharing(population, fitnesses)  # Apply fitness sharing\n",
    "        \n",
    "        new_population = []\n",
    "        # TODO: Elitism saves the top 20 % of the population. Play around with this number and see if you can get a better model\n",
    "        # keep the 20% best genomes\n",
    "        new_population.extend([population[i] for i in np.argsort(fitnesses)[-int(pop_size * 0.2):]])\n",
    "        while len(new_population) < pop_size:\n",
    "            # Select parents\n",
    "            #parent1, parent2 = tournament_selection(population, fitnesses), tournament_selection(population, fitnesses)\n",
    "            parent1 = roulette_wheel_selection(population, fitnesses)\n",
    "            parent2 = roulette_wheel_selection(population, fitnesses)\n",
    "            # Crossover & Mutation\n",
    "            #child1, child2 = uniform_crossover(parent1, parent2)\n",
    "            child1, child2 = crossover(parent1, parent2)\n",
    "            child1, child2 = bit_flip_mutation(child1), bit_flip_mutation(child2)\n",
    "            #diversity_factor = 1 + (np.mean(fitness_sharing(population, fitnesses)) * 0.5)  # Scale mutation if diversity is low\n",
    "            #child1 = adaptive_mutate(child1, gen, generations, diversity_factor=diversity_factor)\n",
    "            #child2 = adaptive_mutate(child2, gen, generations, diversity_factor=diversity_factor)\n",
    "            #child1, child2 = deterministic_crowding(parent1, parent2, child1, child2, evaluate_fitness)\n",
    "            \n",
    "            new_population.extend([child1, child2])\n",
    "\n",
    "        # Replace population (keep best elite)\n",
    "        best_idx = np.argmax(fitnesses)\n",
    "        best_genome = population[best_idx]\n",
    "        population = new_population[:pop_size]\n",
    "        population[0] = best_genome  # Elitism\n",
    "        if population[np.argmax(fitnesses)] == best_genome:\n",
    "            max_generation = population.copy()\n",
    "        \n",
    "        print(f\"Generation {gen + 1}: Best Fitness = {max(fitnesses):.6f} avg fitness = {np.mean(fitnesses):.6f}\")\n",
    "       \n",
    "    \n",
    "    best_genome = max_generation[np.argmax(fitnesses)]\n",
    "    print(f\"Best genome: {best_genome}\")\n",
    "    return best_genome  # Return best genome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network successfully built with shapes:\n",
      "w1: (2, 5), b1: (5,), w2: (5, 1), b2: (1,)\n",
      "Required bitstring length: 660 bits\n",
      "Generation 1: Best Fitness = 32.000000 avg fitness = 17.060000\n",
      "Generation 2: Best Fitness = 44.000000 avg fitness = 24.071946\n",
      "Generation 3: Best Fitness = 71.000000 avg fitness = 27.250000\n",
      "Generation 4: Best Fitness = 539.000000 avg fitness = 33.040216\n",
      "Generation 5: Best Fitness = 1139.673913 avg fitness = 115.617807\n",
      "Generation 6: Best Fitness = 3991.176471 avg fitness = 294.882173\n",
      "Generation 7: Best Fitness = 1082.000000 avg fitness = 262.207601\n",
      "Generation 8: Best Fitness = 2055.000000 avg fitness = 342.213662\n",
      "Generation 9: Best Fitness = 1077.000000 avg fitness = 334.950577\n",
      "Generation 10: Best Fitness = 4150.000000 avg fitness = 434.500233\n",
      "Generation 11: Best Fitness = 2104.000000 avg fitness = 380.095876\n",
      "Generation 12: Best Fitness = 1082.000000 avg fitness = 262.975297\n",
      "Generation 13: Best Fitness = 1081.000000 avg fitness = 307.949938\n",
      "Generation 14: Best Fitness = 3094.500000 avg fitness = 365.519517\n",
      "Generation 15: Best Fitness = 11063.366337 avg fitness = 616.375482\n",
      "Generation 16: Best Fitness = 23299.000000 avg fitness = 1120.259540\n",
      "Generation 17: Best Fitness = 72044.000000 avg fitness = 2896.551628\n",
      "Generation 18: Best Fitness = 7163.000000 avg fitness = 707.305603\n",
      "Generation 19: Best Fitness = 17079.200000 avg fitness = 1514.587175\n",
      "Generation 20: Best Fitness = 12214.000000 avg fitness = 1688.934692\n",
      "Generation 21: Best Fitness = 7203.000000 avg fitness = 1450.417583\n",
      "Generation 22: Best Fitness = 16300.000000 avg fitness = 1753.730186\n",
      "Generation 23: Best Fitness = 18350.000000 avg fitness = 2400.880649\n",
      "Generation 24: Best Fitness = 27276.000000 avg fitness = 2037.021213\n",
      "Generation 25: Best Fitness = 36426.000000 avg fitness = 1983.609438\n",
      "Generation 26: Best Fitness = 83895.000000 avg fitness = 5114.520685\n",
      "Generation 27: Best Fitness = 33556.000000 avg fitness = 4592.094347\n",
      "Generation 28: Best Fitness = 79044.000000 avg fitness = 6954.839016\n",
      "Generation 29: Best Fitness = 77994.117647 avg fitness = 6920.283079\n",
      "Generation 30: Best Fitness = 141920.000000 avg fitness = 12119.387891\n",
      "Generation 31: Best Fitness = 133659.000000 avg fitness = 13185.567416\n",
      "Generation 32: Best Fitness = 138913.000000 avg fitness = 12569.217160\n",
      "Generation 33: Best Fitness = 71024.000000 avg fitness = 10588.796557\n",
      "Generation 34: Best Fitness = 67814.000000 avg fitness = 7398.892701\n",
      "Generation 35: Best Fitness = 95282.000000 avg fitness = 11611.213550\n",
      "Generation 36: Best Fitness = 126929.357798 avg fitness = 12321.033401\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 20\u001b[0m\n\u001b[0;32m     16\u001b[0m required_length \u001b[38;5;241m=\u001b[39m required_bitstring_length(hidden_neurons, input_size, output_size, bits_per_weight) \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequired bitstring length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequired_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bits\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Debugging\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m best_genome \u001b[38;5;241m=\u001b[39m \u001b[43mgenetic_algorithm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpop_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenome_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequired_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 16 parameters × 8 bits\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m save_best_genome(best_genome, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_genome.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[53], line 8\u001b[0m, in \u001b[0;36mgenetic_algorithm\u001b[1;34m(pop_size, genome_length, generations)\u001b[0m\n\u001b[0;32m      5\u001b[0m max_generation \u001b[38;5;241m=\u001b[39m population\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gen \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(generations):\n\u001b[1;32m----> 8\u001b[0m     fitnesses \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([evaluate_fitness(ind) \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m population])\n\u001b[0;32m      9\u001b[0m     fitnesses \u001b[38;5;241m=\u001b[39m fitness_sharing(population, fitnesses)  \u001b[38;5;66;03m# Apply fitness sharing\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     new_population \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[53], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      5\u001b[0m max_generation \u001b[38;5;241m=\u001b[39m population\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gen \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(generations):\n\u001b[1;32m----> 8\u001b[0m     fitnesses \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mevaluate_fitness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mind\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m population])\n\u001b[0;32m      9\u001b[0m     fitnesses \u001b[38;5;241m=\u001b[39m fitness_sharing(population, fitnesses)  \u001b[38;5;66;03m# Apply fitness sharing\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     new_population \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[46], line 19\u001b[0m, in \u001b[0;36mevaluate_fitness\u001b[1;34m(chromosome)\u001b[0m\n\u001b[0;32m     16\u001b[0m     X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;241m1.0\u001b[39m])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# the first observation is not in the correct shape\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Forward pass through the NN\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# TODO: play around with the threshold and see if you can get a better model\u001b[39;00m\n\u001b[0;32m     22\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.7\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[42], line 60\u001b[0m, in \u001b[0;36mforward_pass\u001b[1;34m(X, w1, b1, w2, b2)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward_pass\u001b[39m(X, w1, b1, w2, b2):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Performs forward propagation through the neural network\"\"\"\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtanh(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b1)  \u001b[38;5;66;03m# (batch, hidden)\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtanh(np\u001b[38;5;241m.\u001b[39mdot(hidden, w2) \u001b[38;5;241m+\u001b[39m b2)  \u001b[38;5;66;03m# (batch, 1)\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "random_bitstring = \"\".join(np.random.choice([\"0\", \"1\"], required_bitstring_length(hidden_neurons=11)))\n",
    "\n",
    "try:\n",
    "    w1, b1, w2, b2 = construct_nn(random_bitstring)\n",
    "    print(\"Neural Network successfully built with shapes:\")\n",
    "    print(f\"w1: {w1.shape}, b1: {b1.shape}, w2: {w2.shape}, b2: {b2.shape}\")\n",
    "except ValueError as e:\n",
    "    print(\"Error:\", e)\n",
    "\n",
    "\n",
    "input_size = 2\n",
    "output_size = 1\n",
    "hidden_neurons = 16  # Max number of hidden neurons\n",
    "bits_per_weight = 10\n",
    "\n",
    "required_length = required_bitstring_length(hidden_neurons, input_size, output_size, bits_per_weight) +10\n",
    "\n",
    "print(f\"Required bitstring length: {required_length} bits\")  # Debugging\n",
    "\n",
    "best_genome = genetic_algorithm(\n",
    "    pop_size=50, \n",
    "    genome_length=required_length,  # 16 parameters × 8 bits\n",
    "    generations=50\n",
    "    \n",
    ")\n",
    "save_best_genome(best_genome, \"best_genome.txt\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "construct_nn() got an unexpected keyword argument 'input_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m num_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# The envirement has 1 action space (flap or do nothing)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m best_genome \u001b[38;5;241m=\u001b[39m load_best_genome(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_genome_score_275.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m w1, b1, w2, b2 \u001b[38;5;241m=\u001b[39m \u001b[43mconstruct_nn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_genome\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     10\u001b[0m best_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: construct_nn() got an unexpected keyword argument 'input_size'"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "\n",
    "num_inputs = 2 # The envirement has 12 observations for each frame\n",
    "num_outputs = 1 # The envirement has 1 action space (flap or do nothing)\n",
    "    \n",
    "best_genome = load_best_genome(\"best_genome_score_275.txt\")\n",
    "w1, b1, w2, b2 = construct_nn(best_genome, input_size=num_inputs, output_size=num_outputs)\n",
    "total_reward = 0\n",
    "best_score = 0\n",
    "\n",
    "for i in range(100):\n",
    "    obs, _ = env.reset()\n",
    "    while True:\n",
    "        try:\n",
    "            X = np.array(obs).reshape(1, 2)\n",
    "        except:\n",
    "            X = np.array([0.1,1.0]).reshape(1, 2)\n",
    "        # Forward pass through the NN\n",
    "        output = forward_pass(X, w1, b1, w2, b2)\n",
    "        action = 1 if output > 0.7 else 0\n",
    "        obs, reward, terminated, info = env.step(action)\n",
    "        \n",
    "        if terminated:\n",
    "            break\n",
    "    print(info)\n",
    "    if info['score'] > best_score:\n",
    "        best_score = info['score']\n",
    "    \n",
    "print(\"best score: \", best_score)   \n",
    "\n",
    "env.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# Neural network parameters:\n",
    "# ===============================================================\n",
    "num_inputs = 2 # The envirement has 12 observations for each frame\n",
    "num_outputs = 1 # The envirement has 1 action space (flap or do nothing)\n",
    "    \n",
    "best_genome = load_best_genome(\"best_genome_score_275.txt\")\n",
    "w1, b1, w2, b2 = construct_nn(best_genome, input_size=num_inputs, output_size=num_outputs)\n",
    "obs, _ = env.reset()\n",
    "total_reward = 0\n",
    "while True:\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        X = np.array(obs).reshape(1, 2)\n",
    "    except:\n",
    "        X = np.array([0.1,1.0]).reshape(1, 2)\n",
    "\n",
    "    # Forward pass through the NN\n",
    "    output = forward_pass(X, w1, b1, w2, b2)\n",
    "    action = 1 if output > 0.7 else 0\n",
    "    obs, reward, terminated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "    env.render()\n",
    "    time.sleep(1 / 30)\n",
    "    \n",
    "    if terminated:\n",
    "        break\n",
    "print(info)\n",
    "    \n",
    "  \n",
    "\n",
    "env.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import numpy as np\n",
    "\n",
    "def draw_neural_network(chromosome, screen_size=(800, 600)):\n",
    "    \"\"\"\n",
    "    Draws a neural network based on the given chromosome using Pygame.\n",
    "\n",
    "    Parameters:\n",
    "    - chromosome: Bitstring representing the neural network structure.\n",
    "    - screen_size: Tuple (width, height) for pygame window.\n",
    "    \"\"\"\n",
    "    # Decode the architecture and construct the neural network\n",
    "    hidden_neurons, params = decode_architecture(chromosome)\n",
    "    w1, b1, w2, b2 = construct_nn(chromosome)\n",
    "\n",
    "    # Ensure hidden_neurons is a list\n",
    "    input_neurons = 2  # From the Flappy Bird environment\n",
    "    output_neurons = 1  # Single output neuron\n",
    "    layers = [input_neurons] + ([hidden_neurons] if isinstance(hidden_neurons, int) else hidden_neurons) + [output_neurons]\n",
    "\n",
    "    pygame.init()\n",
    "    screen = pygame.display.set_mode(screen_size)\n",
    "    pygame.display.set_caption(\"Neural Network Visualization\")\n",
    "\n",
    "    # Colors\n",
    "    WHITE = (255, 255, 255)\n",
    "    BLACK = (0, 0, 0)\n",
    "    BLUE = (50, 150, 255)\n",
    "    GREEN = (100, 255, 100)\n",
    "    RED = (255, 50, 50)\n",
    "\n",
    "    screen.fill(WHITE)\n",
    "\n",
    "    # Define spacing\n",
    "    width, height = screen_size\n",
    "    layer_spacing = width // (len(layers) + 1)  # Space between layers\n",
    "    node_radius = 20  # Neuron size\n",
    "    y_padding = 80  # Padding from top/bottom\n",
    "\n",
    "    # Compute node positions\n",
    "    node_positions = []  # List of lists for neuron positions\n",
    "\n",
    "    for i, neurons in enumerate(layers):\n",
    "        x = (i + 1) * layer_spacing\n",
    "        y_spacing = (height - 2 * y_padding) // (neurons - 1) if neurons > 1 else 200\n",
    "\n",
    "        layer_nodes = []\n",
    "        for j in range(neurons):\n",
    "            y = y_padding + j * y_spacing\n",
    "            layer_nodes.append((x, y))\n",
    "\n",
    "        node_positions.append(layer_nodes)\n",
    "\n",
    "    # Draw connections (edges) with weight intensity\n",
    "    for i in range(len(layers) - 1):\n",
    "        weight_matrix = w1 if i == 0 else w2  # Use w1 for input-hidden, w2 for hidden-output\n",
    "        layer1_nodes = node_positions[i]\n",
    "        layer2_nodes = node_positions[i + 1]\n",
    "\n",
    "        for j, node1 in enumerate(layer1_nodes):  # Nodes in layer i\n",
    "            for k, node2 in enumerate(layer2_nodes):  # Nodes in layer i+1\n",
    "                try:\n",
    "                    weight = weight_matrix[k, j]  # Extract weight\n",
    "                    color = GREEN if weight > 0 else RED  # Green for positive, red for negative\n",
    "                    thickness = int(abs(weight) * 5) + 1  # Scale thickness\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "                pygame.draw.line(screen, color, node1, node2, thickness)\n",
    "\n",
    "    # Draw nodes (neurons)\n",
    "    for layer in node_positions:\n",
    "        for node in layer:\n",
    "            \n",
    "            pygame.draw.circle(screen, BLUE, node, node_radius)\n",
    "            pygame.draw.circle(screen, BLACK, node, node_radius, 2)  # Outline\n",
    "\n",
    "    # Display network\n",
    "    pygame.display.flip()\n",
    "\n",
    "    # Wait until user closes the window\n",
    "    running = True\n",
    "    while running:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "\n",
    "    pygame.quit()\n",
    "\n",
    "\n",
    "\n",
    "draw_neural_network(load_best_genome(\"best_genome.txt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
