{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Genetic algorithm for neuroevolution** \n",
    "\n",
    "This notebook aims to find a solution to flappy bird by using a simple genetic algorithm to conduct a large search to fit a neural network. The algorithm is based on genomes consisting of a bitstring, mapping the bits to a neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flappy-bird-gym in c:\\users\\tobia\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: pygame~=2.0.1 in c:\\users\\tobia\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from flappy-bird-gym) (2.0.3)\n",
      "Requirement already satisfied: gym~=0.18.0 in c:\\users\\tobia\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from flappy-bird-gym) (0.18.3)\n",
      "Requirement already satisfied: numpy~=1.19.5 in c:\\users\\tobia\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from flappy-bird-gym) (1.19.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\tobia\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gym~=0.18.0->flappy-bird-gym) (1.10.1)\n",
      "Requirement already satisfied: pyglet<=1.5.15,>=1.4.0 in c:\\users\\tobia\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gym~=0.18.0->flappy-bird-gym) (1.5.15)\n",
      "Requirement already satisfied: Pillow<=8.2.0 in c:\\users\\tobia\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gym~=0.18.0->flappy-bird-gym) (8.2.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\tobia\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gym~=0.18.0->flappy-bird-gym) (1.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 25.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\tobia\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "%pip install flappy-bird-gym\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import flappy_bird_gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a structure organism in order to keep track of the chromosome and the fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Organism:\n",
    "    def __init__(self, chromosome, fitness):\n",
    "        self.chromosome = \"\"\n",
    "        self.fitness = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains functions for converting the chromosome into a neural network, as well as useful functions like forward pass in order to let the creatures make predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL!\n",
    "\n",
    "def bitstring_to_floats(bitstring, bits_per_value=10, min_val=-2, max_val=2):\n",
    "    \"\"\"Converts a bitstring into an array of float values while ensuring full extraction.\"\"\"\n",
    "    num_values = len(bitstring) // bits_per_value  # Ensure full number of weights\n",
    "    if num_values == 0:\n",
    "        raise ValueError(\"Bitstring too short for any weights!\")\n",
    "\n",
    "    floats = []\n",
    "    for i in range(num_values):\n",
    "        binary_segment = bitstring[i * bits_per_value:(i + 1) * bits_per_value]\n",
    "        decimal_value = int(binary_segment, 2)  # Convert binary to decimal\n",
    "        scaled_value = min_val + (max_val - min_val) * (decimal_value / (2**bits_per_value - 1))  # Normalize to [-2,2]\n",
    "        floats.append(scaled_value)\n",
    "\n",
    "    return np.array(floats)\n",
    "\n",
    "\n",
    "def decode_architecture(bitstring, bits_per_weight=10, input_size=2, output_size=1):\n",
    "    \"\"\"Extracts NN structure (neurons) and weights from a bitstring\"\"\"\n",
    "    hidden_neurons = int(bitstring[:4], 2) + 1  # Allow 1-16 hidden neurons\n",
    "    weights = bitstring_to_floats(bitstring[4:], bits_per_value=bits_per_weight)\n",
    "\n",
    "    # Compute expected number of weights\n",
    "    required_params = (input_size * hidden_neurons) + hidden_neurons + (hidden_neurons * output_size) + output_size\n",
    "\n",
    "    if len(weights) < required_params:\n",
    "        raise ValueError(f\"Decoded weights ({len(weights)}) are fewer than expected ({required_params}). \"\n",
    "                         f\"Ensure bitstring is at least {required_params * bits_per_weight} bits long!\")\n",
    "\n",
    "    return hidden_neurons, weights\n",
    "\n",
    "\n",
    "def construct_nn(bitstring, input_size=2, output_size=1):\n",
    "    \"\"\"Constructs a simple 1-layer NN from a bitstring with dynamic hidden neurons\"\"\"\n",
    "    hidden_neurons, params = decode_architecture(bitstring, input_size=input_size, output_size=output_size)\n",
    "\n",
    "    # Compute parameter indices\n",
    "    w1_end = input_size * hidden_neurons\n",
    "    b1_end = w1_end + hidden_neurons\n",
    "    w2_end = b1_end + (hidden_neurons * output_size)\n",
    "\n",
    "    # Debugging: Print out parameter sizes\n",
    "    # print(f\"Params Length: {len(params)} | Expected: {w2_end + output_size}\")\n",
    "    # print(f\"Hidden Neurons: {hidden_neurons}, Input Size: {input_size}, Output Size: {output_size}\")\n",
    "    # print(f\"w1: (0:{w1_end}), b1: ({w1_end}:{b1_end}), w2: ({b1_end}:{w2_end}), b2: ({w2_end}:{w2_end + output_size})\")\n",
    "\n",
    "    # Extract weights and biases\n",
    "    w1 = params[:w1_end].reshape(input_size, hidden_neurons)  # (input_size, hidden_neurons)\n",
    "    b1 = params[w1_end:b1_end]  # (hidden_neurons,)\n",
    "    w2 = params[b1_end:w2_end].reshape(hidden_neurons, output_size)  # (hidden_neurons, output_size)\n",
    "    b2 = params[w2_end:w2_end + output_size]  # (output_size,)\n",
    "\n",
    "    return w1, b1, w2, b2\n",
    "\n",
    "\n",
    "\n",
    "def forward_pass(X, w1, b1, w2, b2):\n",
    "    \"\"\"Performs forward propagation through the neural network\"\"\"\n",
    "    hidden = np.tanh(np.dot(X, w1) + b1)  # (batch, hidden)\n",
    "    output = np.tanh(np.dot(hidden, w2) + b2)  # (batch, 1)\n",
    "\n",
    "    return output\n",
    "\n",
    "def required_bitstring_length(hidden_neurons=16, input_size=2, output_size=1, bits_per_weight=10):\n",
    "    \"\"\"Compute the required bitstring length for encoding all NN parameters.\"\"\"\n",
    "    required_params = (input_size * hidden_neurons) + hidden_neurons + (hidden_neurons * output_size) + output_size\n",
    "    return required_params * bits_per_weight  # Convert to bitstring length\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load best genome of a run\n",
    "def save_best_genome(genome, filename):\n",
    "    \"\"\"Saves the best genome to a file\"\"\"\n",
    "    \n",
    "    with open (filename, 'w') as f:\n",
    "        f.write(genome)\n",
    "        \n",
    "def load_best_genome(filename):\n",
    "    \"\"\"Loads the best genome from a file\"\"\"\n",
    "    \n",
    "    with open (filename, 'r') as f:\n",
    "        genome = f.read()\n",
    "        \n",
    "    return genome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize population**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_population(pop_size, genome_length):\n",
    "    \"\"\"Initializes a population of genomes\"\"\"\n",
    "    return[\"\".join(np.random.choice([\"0\", \"1\"], genome_length)) for _ in range(pop_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitness Function**\n",
    "The fitness of a genome is calculated in a fitness function. This often the only domain centric part of a genetic algorithm, meaning that the neural network will be trained based on what kind of environment the fitness function uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "\n",
    "def evaluate_fitness(chromosome):\n",
    "    \"\"\"Evaluates a chromosome based on its fitness.\"\"\"\n",
    "    hidden_neurons, params = decode_architecture(chromosome)\n",
    "    w1, b1, w2, b2 = construct_nn(chromosome)\n",
    "\n",
    "    # Run the game\n",
    "    obs, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        \n",
    "        try:\n",
    "            X = np.array(obs).reshape(1, 2)   \n",
    "        except:\n",
    "            X = np.array([0.1,1.0]).reshape(1, 2) # the first observation is not in the correct shape\n",
    "\n",
    "        # Forward pass through the NN\n",
    "        output = forward_pass(X, w1, b1, w2, b2)\n",
    "        \n",
    "        # TODO: play around with the threshold and see if you can get a better model\n",
    "        action = 1 if output > 0.7 else 0\n",
    "\n",
    "        # Take the action\n",
    "        obs, reward, terminated, info = env.step(action)\n",
    "        total_reward += reward - action * 7\n",
    "        \n",
    "        # Check if the game is over\n",
    "        if terminated:\n",
    "            break\n",
    "        \n",
    "    # clamp reward to be at least 0.000001\n",
    "    return max(total_reward, 0.000001) +  + int(info['score'])*1000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crossover**\n",
    "The crossover function decides how different genomes should mate in order to produce offspring. \n",
    "The gene representation is in the form of a bitstring.\n",
    "\n",
    "**Task** \n",
    "Define your own set of crossover functions by implementing different policies to combine the bitstirng of two parents. All genomes are the same length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_crossover(parent1, parent2):\n",
    "    \"\"\"Uniform crossover for binary genomes\"\"\"\n",
    "    mask = np.random.randint(2, size=len(parent1))  # Random bit mask\n",
    "    child1 = \"\".join([parent1[i] if mask[i] else parent2[i] for i in range(len(parent1))])\n",
    "    child2 = \"\".join([parent2[i] if mask[i] else parent1[i] for i in range(len(parent1))])\n",
    "    return child1, child2\n",
    "\n",
    "\n",
    "def single_point_crossover(parent1, parent2, crossover_rate=0.9):\n",
    "    \"\"\"Single-point crossover for binary genomes\"\"\"\n",
    "    if np.random.rand() > crossover_rate:\n",
    "        return parent1, parent2\n",
    "    split_point = np.random.randint(1, len(parent1) - 1)  # Random split point\n",
    "    child1 = parent1[:split_point] + parent2[split_point:]\n",
    "    child2 = parent2[:split_point] + parent1[split_point:]\n",
    "    return child1, child2\n",
    "\n",
    "\n",
    "def crossover(parent1, parent2, crossover_rate=0.9):\n",
    "    random_number = np.random.rand()\n",
    "    if random_number < 0.5:\n",
    "        return single_point_crossover(parent1, parent2, crossover_rate)\n",
    "    else:\n",
    "        return uniform_crossover(parent1, parent2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bit_flip_mutation(bitstring, mutation_rate=0.01):\n",
    "    \"\"\"Flips random bits with given probability\"\"\"\n",
    "    return \"\".join([bit if np.random.rand() > mutation_rate else str(1 - int(bit)) for bit in bitstring])\n",
    "\n",
    "\n",
    "def adaptive_mutate(chromosome, generation, max_generations, mutation_rate=0.01, diversity_factor=1):\n",
    "    \"\"\"Adapts mutation rate based on population diversity.\"\"\"\n",
    "    adjusted_mutation_rate = mutation_rate * (1 - (generation / max_generations)) * diversity_factor\n",
    "\n",
    "    chromosome_list = list(chromosome)  # Convert to mutable list\n",
    "    for i in range(len(chromosome_list)):\n",
    "        if np.random.rand() < adjusted_mutation_rate:\n",
    "            chromosome_list[i] = '1' if chromosome_list[i] == '0' else '0'  # Flip bit\n",
    "\n",
    "    return \"\".join(chromosome_list)  # Convert back to string\n",
    "\n",
    "\n",
    "def swap_mutation(chromosome):\n",
    "    \"\"\"Performs swap mutation on a given chromosome by swapping two random genes.\"\"\"\n",
    "    if len(chromosome) < 2:\n",
    "        return chromosome  # No mutation possible\n",
    "    \n",
    "    idx1, idx2 = random.sample(range(len(chromosome)), 2)\n",
    "    chromosome[idx1], chromosome[idx2] = chromosome[idx2], chromosome[idx1]\n",
    "    \n",
    "    return chromosome\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deterministic_crowding(parent1, parent2, child1, child2, fitness_function):\n",
    "    \"\"\"Ensures offspring compete with similar parents.\"\"\"\n",
    "    f_p1 = fitness_function(parent1)\n",
    "    f_p2 = fitness_function(parent2)\n",
    "    f_c1 = fitness_function(child1)\n",
    "    f_c2 = fitness_function(child2)\n",
    "\n",
    "    new1 = child1 if f_c1 > f_p1 else parent1\n",
    "    new2 = child2 if f_c2 > f_p2 else parent2\n",
    "\n",
    "    return new1, new2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_distance(bitstring1, bitstring2):\n",
    "    \"\"\"Calculates Hamming distance between two bitstrings.\"\"\"\n",
    "    return sum(b1 != b2 for b1, b2 in zip(bitstring1, bitstring2))\n",
    "\n",
    "\n",
    "def fitness_sharing(population, fitnesses, sigma_share=10, alpha=2):\n",
    "    \"\"\"Applies fitness sharing to promote diversity.\"\"\"\n",
    "    shared_fitnesses = np.zeros(len(fitnesses))\n",
    "\n",
    "    for i in range(len(population)):\n",
    "        niche_count = 0\n",
    "        for j in range(len(population)):\n",
    "            if i != j:\n",
    "                distance = hamming_distance(population[i], population[j])\n",
    "                if distance < sigma_share:  # If genomes are similar\n",
    "                    niche_count += (1 - (distance / sigma_share)) ** alpha\n",
    "\n",
    "        shared_fitnesses[i] = fitnesses[i] / (1 + niche_count)  # Penalize common solutions\n",
    "\n",
    "    return shared_fitnesses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tournament_selection(population, fitnesses, tournament_size=10):\n",
    "    \"\"\"Select best genome from a random subset\"\"\"\n",
    "    indices = np.random.choice(len(population), tournament_size, replace=False)\n",
    "    best_index = indices[np.argmax([fitnesses[i] for i in indices])]\n",
    "    return population[best_index]\n",
    "\n",
    "\n",
    "def roulette_wheel_selection(population, fitnesses):\n",
    "    \"\"\"Selects individuals using fitness proportionate selection.\"\"\"\n",
    "    total_fitness = np.sum(fitnesses)\n",
    "    probabilities = fitnesses / total_fitness\n",
    "    return population[np.random.choice(len(population), p=probabilities)]\n",
    "\n",
    "\n",
    "def rank_based_selection(population):\n",
    "    \"\"\"Selects a parent based on rank selection.\"\"\"\n",
    "    population_sorted = sorted(population, key=lambda org: org.fitness)\n",
    "    ranks = np.arange(1, len(population) + 1)\n",
    "    probabilities = ranks / ranks.sum()\n",
    "    return np.random.choice(population_sorted, p=probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genetic_algorithm(pop_size, genome_length, generations):\n",
    "    \"\"\"Runs a genetic algorithm to evolve a neural network\"\"\"\n",
    "    # Initialize population (random bitstrings)\n",
    "    population = initialize_population(pop_size, genome_length)\n",
    "    max_generation = population\n",
    "\n",
    "    for gen in range(generations):\n",
    "        fitnesses = np.array([evaluate_fitness(ind) for ind in population])\n",
    "        fitnesses = fitness_sharing(population, fitnesses)  # Apply fitness sharing\n",
    "        \n",
    "        new_population = []\n",
    "        # TODO: Elitism saves the top 20 % of the population. Play around with this number and see if you can get a better model\n",
    "        # keep the 20% best genomes\n",
    "        new_population.extend([population[i] for i in np.argsort(fitnesses)[-int(pop_size * 0.2):]])\n",
    "        while len(new_population) < pop_size:\n",
    "            # Select parents\n",
    "            #parent1, parent2 = tournament_selection(population, fitnesses), tournament_selection(population, fitnesses)\n",
    "            parent1 = roulette_wheel_selection(population, fitnesses)\n",
    "            parent2 = roulette_wheel_selection(population, fitnesses)\n",
    "            # Crossover & Mutation\n",
    "            #child1, child2 = uniform_crossover(parent1, parent2)\n",
    "            child1, child2 = crossover(parent1, parent2)\n",
    "            child1, child2 = bit_flip_mutation(child1), bit_flip_mutation(child2)\n",
    "            #diversity_factor = 1 + (np.mean(fitness_sharing(population, fitnesses)) * 0.5)  # Scale mutation if diversity is low\n",
    "            #child1 = adaptive_mutate(child1, gen, generations, diversity_factor=diversity_factor)\n",
    "            #child2 = adaptive_mutate(child2, gen, generations, diversity_factor=diversity_factor)\n",
    "            #child1, child2 = deterministic_crowding(parent1, parent2, child1, child2, evaluate_fitness)\n",
    "            \n",
    "            new_population.extend([child1, child2])\n",
    "\n",
    "        # Replace population (keep best elite)\n",
    "        best_idx = np.argmax(fitnesses)\n",
    "        best_genome = population[best_idx]\n",
    "        population = new_population[:pop_size]\n",
    "        population[0] = best_genome  # Elitism\n",
    "        if population[np.argmax(fitnesses)] == best_genome:\n",
    "            max_generation = population.copy()\n",
    "        \n",
    "        print(f\"Generation {gen + 1}: Best Fitness = {max(fitnesses):.6f} avg fitness = {np.mean(fitnesses):.6f}\")\n",
    "       \n",
    "    \n",
    "    best_genome = max_generation[np.argmax(fitnesses)]\n",
    "    print(f\"Best genome: {best_genome}\")\n",
    "    return best_genome  # Return best genome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Decoded weights (44) are fewer than expected (65). Ensure bitstring is at least 650 bits long!\n",
      "Required bitstring length: 660 bits\n",
      "Generation 1: Best Fitness = 32.000000 avg fitness = 21.500000\n",
      "Generation 2: Best Fitness = 45.000000 avg fitness = 23.023449\n",
      "Generation 3: Best Fitness = 68.000000 avg fitness = 25.396054\n",
      "Generation 4: Best Fitness = 68.000000 avg fitness = 27.390000\n",
      "Generation 5: Best Fitness = 73.000000 avg fitness = 25.832673\n",
      "Generation 6: Best Fitness = 73.000000 avg fitness = 29.254561\n",
      "Generation 7: Best Fitness = 73.000000 avg fitness = 31.813765\n",
      "Generation 8: Best Fitness = 73.000000 avg fitness = 29.056406\n",
      "Generation 9: Best Fitness = 68.000000 avg fitness = 26.858788\n",
      "Generation 10: Best Fitness = 73.000000 avg fitness = 32.067209\n",
      "Generation 11: Best Fitness = 73.000000 avg fitness = 32.961106\n",
      "Generation 12: Best Fitness = 73.000000 avg fitness = 35.435524\n",
      "Generation 13: Best Fitness = 73.000000 avg fitness = 38.270000\n",
      "Generation 14: Best Fitness = 73.000000 avg fitness = 43.678165\n",
      "Generation 15: Best Fitness = 73.000000 avg fitness = 42.060000\n",
      "Generation 16: Best Fitness = 73.000000 avg fitness = 40.636923\n",
      "Generation 17: Best Fitness = 73.000000 avg fitness = 47.053216\n",
      "Generation 18: Best Fitness = 1078.000000 avg fitness = 62.431172\n",
      "Generation 19: Best Fitness = 1083.000000 avg fitness = 133.982811\n",
      "Generation 20: Best Fitness = 1078.000000 avg fitness = 221.839878\n",
      "Generation 21: Best Fitness = 2000.000001 avg fitness = 337.064713\n",
      "Generation 22: Best Fitness = 1082.000000 avg fitness = 323.904507\n",
      "Generation 23: Best Fitness = 5106.000000 avg fitness = 354.926365\n",
      "Generation 24: Best Fitness = 6927.966102 avg fitness = 607.620886\n",
      "Generation 25: Best Fitness = 12112.000000 avg fitness = 1029.808068\n",
      "Generation 26: Best Fitness = 22207.000000 avg fitness = 2435.432148\n",
      "Generation 27: Best Fitness = 89158.000000 avg fitness = 7124.870489\n",
      "Generation 28: Best Fitness = 86479.000000 avg fitness = 14096.999393\n",
      "Generation 29: Best Fitness = 166717.592593 avg fitness = 20815.739426\n",
      "Generation 30: Best Fitness = 163642.000000 avg fitness = 31014.034267\n",
      "Generation 31: Best Fitness = 359147.000000 avg fitness = 25115.220000\n",
      "Generation 32: Best Fitness = 146649.000000 avg fitness = 19193.579523\n",
      "Generation 33: Best Fitness = 99416.000000 avg fitness = 21693.750311\n",
      "Generation 34: Best Fitness = 126398.148148 avg fitness = 19729.498347\n",
      "Generation 35: Best Fitness = 87994.000000 avg fitness = 15892.814700\n",
      "Generation 36: Best Fitness = 203451.485149 avg fitness = 31656.331343\n",
      "Generation 37: Best Fitness = 159454.000000 avg fitness = 21291.010881\n",
      "Generation 38: Best Fitness = 214400.000000 avg fitness = 28689.154000\n",
      "Generation 39: Best Fitness = 97192.000000 avg fitness = 16674.218805\n",
      "Generation 40: Best Fitness = 238952.000000 avg fitness = 23542.227674\n",
      "Generation 41: Best Fitness = 221629.000000 avg fitness = 21987.738613\n",
      "Generation 42: Best Fitness = 242334.000000 avg fitness = 30224.922179\n",
      "Generation 43: Best Fitness = 92282.000000 avg fitness = 16339.707015\n",
      "Generation 44: Best Fitness = 116302.000000 avg fitness = 18747.346000\n",
      "Generation 45: Best Fitness = 118418.000000 avg fitness = 17851.576485\n",
      "Generation 46: Best Fitness = 334407.000000 avg fitness = 31901.941825\n",
      "Generation 47: Best Fitness = 155668.000000 avg fitness = 20583.622000\n",
      "Generation 48: Best Fitness = 303239.000000 avg fitness = 48026.519358\n",
      "Generation 49: Best Fitness = 144652.000000 avg fitness = 21336.112816\n",
      "Generation 50: Best Fitness = 155920.000000 avg fitness = 28439.822482\n",
      "Best genome: 010111111010110100110000001111000101000011000110011011001110001011100101101010001011110100000110011111010100000011000011001100110000100011000001101110111000111101001001011011110100100111100010111011000100101101010101000010101110110100100011000000100011100101101000001111101011111100001111001101010100111100001101110101001100110111011010010110010101100010101000100010001000010100001010010110011000110001100010000011100000111100110011111011011010110100001000000101100110111001111000011010110110101111010101100111001001110011101000001010011011000111011100111001001101100011101000001111001010110110110001000101010000001100111000000101101110001111101111011000111011\n"
     ]
    }
   ],
   "source": [
    "random_bitstring = \"\".join(np.random.choice([\"0\", \"1\"], required_bitstring_length(hidden_neurons=11)))\n",
    "\n",
    "try:\n",
    "    w1, b1, w2, b2 = construct_nn(random_bitstring)\n",
    "    print(\"Neural Network successfully built with shapes:\")\n",
    "    print(f\"w1: {w1.shape}, b1: {b1.shape}, w2: {w2.shape}, b2: {b2.shape}\")\n",
    "except ValueError as e:\n",
    "    print(\"Error:\", e)\n",
    "\n",
    "\n",
    "input_size = 2\n",
    "output_size = 1\n",
    "hidden_neurons = 16  # Max number of hidden neurons\n",
    "bits_per_weight = 10\n",
    "\n",
    "required_length = required_bitstring_length(hidden_neurons, input_size, output_size, bits_per_weight) +10\n",
    "\n",
    "print(f\"Required bitstring length: {required_length} bits\")  # Debugging\n",
    "\n",
    "best_genome = genetic_algorithm(\n",
    "    pop_size=50, \n",
    "    genome_length=required_length,  # 16 parameters × 8 bits\n",
    "    generations=50\n",
    "    \n",
    ")\n",
    "save_best_genome(best_genome, \"best_genome.txt\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 11}\n",
      "{'score': 131}\n",
      "{'score': 99}\n",
      "{'score': 23}\n",
      "{'score': 34}\n",
      "{'score': 26}\n",
      "{'score': 79}\n",
      "{'score': 57}\n",
      "{'score': 237}\n",
      "{'score': 43}\n",
      "{'score': 91}\n",
      "{'score': 103}\n",
      "{'score': 3}\n",
      "{'score': 120}\n",
      "{'score': 111}\n",
      "{'score': 2}\n",
      "{'score': 32}\n",
      "{'score': 54}\n",
      "{'score': 213}\n",
      "{'score': 8}\n",
      "{'score': 126}\n",
      "{'score': 199}\n",
      "{'score': 65}\n",
      "{'score': 53}\n",
      "{'score': 91}\n",
      "{'score': 3}\n",
      "{'score': 78}\n",
      "{'score': 160}\n",
      "{'score': 22}\n",
      "{'score': 209}\n",
      "{'score': 254}\n",
      "{'score': 42}\n",
      "{'score': 33}\n",
      "{'score': 170}\n",
      "{'score': 281}\n",
      "{'score': 57}\n",
      "{'score': 70}\n",
      "{'score': 87}\n",
      "{'score': 31}\n",
      "{'score': 307}\n",
      "{'score': 125}\n",
      "{'score': 248}\n",
      "{'score': 113}\n",
      "{'score': 41}\n",
      "{'score': 50}\n",
      "{'score': 30}\n",
      "{'score': 50}\n",
      "{'score': 78}\n",
      "{'score': 170}\n",
      "{'score': 19}\n",
      "{'score': 115}\n",
      "{'score': 103}\n",
      "{'score': 212}\n",
      "{'score': 161}\n",
      "{'score': 64}\n",
      "{'score': 10}\n",
      "{'score': 14}\n",
      "{'score': 103}\n",
      "{'score': 29}\n",
      "{'score': 11}\n",
      "{'score': 129}\n",
      "{'score': 79}\n",
      "{'score': 19}\n",
      "{'score': 33}\n",
      "{'score': 262}\n",
      "{'score': 405}\n",
      "{'score': 18}\n",
      "{'score': 377}\n",
      "{'score': 289}\n",
      "{'score': 857}\n",
      "{'score': 81}\n",
      "{'score': 16}\n",
      "{'score': 143}\n",
      "{'score': 65}\n",
      "{'score': 55}\n",
      "{'score': 153}\n",
      "{'score': 14}\n",
      "{'score': 145}\n",
      "{'score': 47}\n",
      "{'score': 415}\n",
      "{'score': 297}\n",
      "{'score': 171}\n",
      "{'score': 70}\n",
      "{'score': 41}\n",
      "{'score': 102}\n",
      "{'score': 177}\n",
      "{'score': 7}\n",
      "{'score': 48}\n",
      "{'score': 20}\n",
      "{'score': 60}\n",
      "{'score': 144}\n",
      "{'score': 153}\n",
      "{'score': 130}\n",
      "{'score': 71}\n",
      "{'score': 37}\n",
      "{'score': 63}\n",
      "{'score': 133}\n",
      "{'score': 341}\n",
      "{'score': 93}\n",
      "{'score': 152}\n",
      "best score:  857\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "\n",
    "num_inputs = 2 # The envirement has 12 observations for each frame\n",
    "num_outputs = 1 # The envirement has 1 action space (flap or do nothing)\n",
    "    \n",
    "best_genome = load_best_genome(\"best_genome_score_275.txt\")\n",
    "w1, b1, w2, b2 = construct_nn(best_genome, input_size=num_inputs, output_size=num_outputs)\n",
    "total_reward = 0\n",
    "best_score = 0\n",
    "\n",
    "for i in range(100):\n",
    "    obs, _ = env.reset()\n",
    "    while True:\n",
    "        try:\n",
    "            X = np.array(obs).reshape(1, 2)\n",
    "        except:\n",
    "            X = np.array([0.1,1.0]).reshape(1, 2)\n",
    "        # Forward pass through the NN\n",
    "        output = forward_pass(X, w1, b1, w2, b2)\n",
    "        action = 1 if output > 0.7 else 0\n",
    "        obs, reward, terminated, info = env.step(action)\n",
    "        \n",
    "        if terminated:\n",
    "            break\n",
    "    print(info)\n",
    "    if info['score'] > best_score:\n",
    "        best_score = info['score']\n",
    "    \n",
    "print(\"best score: \", best_score)   \n",
    "\n",
    "env.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# Neural network parameters:\n",
    "# ===============================================================\n",
    "num_inputs = 2 # The envirement has 12 observations for each frame\n",
    "num_outputs = 1 # The envirement has 1 action space (flap or do nothing)\n",
    "    \n",
    "best_genome = load_best_genome(\"best_genome_score_275.txt\")\n",
    "w1, b1, w2, b2 = construct_nn(best_genome, input_size=num_inputs, output_size=num_outputs)\n",
    "obs, _ = env.reset()\n",
    "total_reward = 0\n",
    "while True:\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        X = np.array(obs).reshape(1, 2)\n",
    "    except:\n",
    "        X = np.array([0.1,1.0]).reshape(1, 2)\n",
    "\n",
    "    # Forward pass through the NN\n",
    "    output = forward_pass(X, w1, b1, w2, b2)\n",
    "    action = 1 if output > 0.7 else 0\n",
    "    obs, reward, terminated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "    env.render()\n",
    "    time.sleep(1 / 30)\n",
    "    \n",
    "    if terminated:\n",
    "        break\n",
    "print(info)\n",
    "    \n",
    "  \n",
    "\n",
    "env.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import numpy as np\n",
    "\n",
    "def draw_neural_network(chromosome, screen_size=(800, 600)):\n",
    "    \"\"\"\n",
    "    Draws a neural network based on the given chromosome using Pygame.\n",
    "\n",
    "    Parameters:\n",
    "    - chromosome: Bitstring representing the neural network structure.\n",
    "    - screen_size: Tuple (width, height) for pygame window.\n",
    "    \"\"\"\n",
    "    # Decode the architecture and construct the neural network\n",
    "    hidden_neurons, params = decode_architecture(chromosome)\n",
    "    w1, b1, w2, b2 = construct_nn(chromosome)\n",
    "\n",
    "    # Ensure hidden_neurons is a list\n",
    "    input_neurons = 2  # From the Flappy Bird environment\n",
    "    output_neurons = 1  # Single output neuron\n",
    "    layers = [input_neurons] + ([hidden_neurons] if isinstance(hidden_neurons, int) else hidden_neurons) + [output_neurons]\n",
    "\n",
    "    pygame.init()\n",
    "    screen = pygame.display.set_mode(screen_size)\n",
    "    pygame.display.set_caption(\"Neural Network Visualization\")\n",
    "\n",
    "    # Colors\n",
    "    WHITE = (255, 255, 255)\n",
    "    BLACK = (0, 0, 0)\n",
    "    BLUE = (50, 150, 255)\n",
    "    GREEN = (100, 255, 100)\n",
    "    RED = (255, 50, 50)\n",
    "\n",
    "    screen.fill(WHITE)\n",
    "\n",
    "    # Define spacing\n",
    "    width, height = screen_size\n",
    "    layer_spacing = width // (len(layers) + 1)  # Space between layers\n",
    "    node_radius = 20  # Neuron size\n",
    "    y_padding = 80  # Padding from top/bottom\n",
    "\n",
    "    # Compute node positions\n",
    "    node_positions = []  # List of lists for neuron positions\n",
    "\n",
    "    for i, neurons in enumerate(layers):\n",
    "        x = (i + 1) * layer_spacing\n",
    "        y_spacing = (height - 2 * y_padding) // (neurons - 1) if neurons > 1 else 200\n",
    "\n",
    "        layer_nodes = []\n",
    "        for j in range(neurons):\n",
    "            y = y_padding + j * y_spacing\n",
    "            layer_nodes.append((x, y))\n",
    "\n",
    "        node_positions.append(layer_nodes)\n",
    "\n",
    "    # Draw connections (edges) with weight intensity\n",
    "    for i in range(len(layers) - 1):\n",
    "        weight_matrix = w1 if i == 0 else w2  # Use w1 for input-hidden, w2 for hidden-output\n",
    "        layer1_nodes = node_positions[i]\n",
    "        layer2_nodes = node_positions[i + 1]\n",
    "\n",
    "        for j, node1 in enumerate(layer1_nodes):  # Nodes in layer i\n",
    "            for k, node2 in enumerate(layer2_nodes):  # Nodes in layer i+1\n",
    "                try:\n",
    "                    weight = weight_matrix[k, j]  # Extract weight\n",
    "                    color = GREEN if weight > 0 else RED  # Green for positive, red for negative\n",
    "                    thickness = int(abs(weight) * 5) + 1  # Scale thickness\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "                pygame.draw.line(screen, color, node1, node2, thickness)\n",
    "\n",
    "    # Draw nodes (neurons)\n",
    "    for layer in node_positions:\n",
    "        for node in layer:\n",
    "            \n",
    "            pygame.draw.circle(screen, BLUE, node, node_radius)\n",
    "            pygame.draw.circle(screen, BLACK, node, node_radius, 2)  # Outline\n",
    "\n",
    "    # Display network\n",
    "    pygame.display.flip()\n",
    "\n",
    "    # Wait until user closes the window\n",
    "    running = True\n",
    "    while running:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "\n",
    "    pygame.quit()\n",
    "\n",
    "\n",
    "\n",
    "draw_neural_network(load_best_genome(\"best_genome.txt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
